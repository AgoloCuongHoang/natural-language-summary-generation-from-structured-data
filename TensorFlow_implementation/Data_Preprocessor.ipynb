{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll preprocess the data and generate a plug_and_play pickle file for it\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: basic preprocessing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for pickling the data\n",
    "import cPickle as pickle\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# import the Text preprocessing helper to obtain the lists of field_name:content_word pairs\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.utils import *\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for the preprocessing script\n",
    "train_percentage = 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the related files and properly structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_content_words, field_words, content_words = prepare_input_data(data_files_paths['table_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Field_content_words: ', ['name hui', 'name jun', 'image <none>', 'imagesize <none>', 'caption <none>', 'fullname hui', 'fullname jun', 'education <none>', 'nationality <none>', 'playingstyle <none>', 'birthdate <none>', 'birthplace <none>', 'deathdate <none>', 'deathplace <none>', 'height <none>', 'weight <none>', 'medaltemplates <none>', 'articletitle hui', 'articletitle jun'])\n",
      "('Field_words: ', ['type', 'name', 'name', 'name', 'name', 'title', 'title', 'title', 'title', 'title'])\n",
      "('Content_words: ', ['pope', 'michael', 'iii', 'of', 'alexandria', '56th', 'pope', 'of', 'alexandria', '&'])\n"
     ]
    }
   ],
   "source": [
    "# check if all the three lists are proper by printing them out\n",
    "print(\"Field_content_words: \", field_content_words[1])\n",
    "print(\"Field_words: \", field_words[:10])\n",
    "print(\"Content_words: \", content_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 19, 99, 53, 40, 25, 59, 57, 37, 76]\n"
     ]
    }
   ],
   "source": [
    "# extract only the lenghts of the field_content_words and delete the field_content_words in order \n",
    "# to free up resources\n",
    "pair_lengths = map(lambda x: len(x), field_content_words)\n",
    "print(pair_lengths)\n",
    "del field_content_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_sentences = prepare_input_labels(data_files_paths['nb_sentences'], data_files_paths['train_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- . in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community . this building was at one time believed to have later become the site of the cairo geniza . <eos>\n",
      "\n",
      "<start> hui jun is a male former table tennis player from china . <eos>\n",
      "\n",
      "<start> okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer . he currently plays as a striker for yeni malatyaspor . <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label_sentences are concatenated properly to obtain the decoder sentences.\n",
    "for sent in label_sentences[:3]: print(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_field, field_dict, rev_field_dict, vocab_size_field = prepare_tokenizer(field_words)\n",
    "train_data_content, content_dict, rev_content_dict, vocab_size_content = prepare_tokenizer(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 106, 106)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[62],\n",
       "        [ 2],\n",
       "        [ 2]]), array([[11],\n",
       "        [37],\n",
       "        [38]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_size_field, len(rev_field_dict), len(field_dict))\n",
    "train_data_field[:3], train_data_content[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the group function to bring the data together:\n",
    "field_seq = np.squeeze(train_data_field).tolist()\n",
    "content_seq = np.squeeze(train_data_content).tolist()\n",
    "field_sequences, content_sequences = (group_tokenized_sequences(field_seq, pair_lengths),\n",
    "                                         group_tokenized_sequences(content_seq, pair_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 7, 5, 37, 37, 37, 38, 38, 38, 31, 31, 27, 27, 63, 64, 4, 65, 3, 18, 18, 18, 23, 23, 23, 23, 23, 23, 50, 32, 32, 32, 20, 20, 20, 20, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 28, 66, 6, 6, 6, 6, 6], [2, 2, 7, 33, 5, 29, 29, 67, 50, 68, 4, 3, 18, 9, 39, 51, 52, 6, 6]]\n",
      "[[11, 37, 38, 4, 20, 83, 11, 4, 20, 84, 85, 4, 7, 86, 4, 87, 39, 1, 1, 40, 8, 88, 12, 13, 41, 89, 42, 90, 42, 1, 1, 1, 1, 91, 12, 13, 41, 92, 4, 43, 93, 7, 94, 95, 44, 96, 97, 43, 39, 21, 45, 12, 13, 5, 14, 98, 22, 7, 44, 99, 6, 1, 1, 11, 37, 38, 4, 20], [23, 24, 1, 1, 1, 23, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "# print some slices of the field_sequences and the content_sequences:\n",
    "print(field_sequences[:2])\n",
    "print(content_sequences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the defined pad_sequences function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of padded_sequences: ', (10, 99))\n"
     ]
    }
   ],
   "source": [
    "padded_field_sequences = pad_sequences(field_sequences)\n",
    "print(\"Length of padded_sequences: \", padded_field_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform structuring of the label_sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: convert the label_sentences into a single flat list (order preserved) in order to tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[450, 71, 155]\n"
     ]
    }
   ],
   "source": [
    "# extract the length information from the label_sentences\n",
    "label_sentences_lengths = map(lambda x: len(x), label_sentences)\n",
    "print(label_sentences_lengths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'pope', 'michael', 'iii', 'of', 'alexandria', '-lrb-', 'also', 'known', 'as']\n"
     ]
    }
   ],
   "source": [
    "''' Warning: This is a huge map - reduce operation. And may take a long time to execute '''\n",
    "label_words_list = reduce(lambda x,y: x + y, map(lambda x: x.split(), label_sentences))\n",
    "print(label_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Use the earlier defined function prepare_tokenizer to transform the input words into numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tokenizer function to obtain the token and processed data for label_sentences\n",
    "train_data_label, label_dict, rev_label_dict, vocab_size_label = prepare_tokenizer(label_sentences)\n",
    "train_data_label = train_data_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 32, 33, 34, 4, 35, 5, 47, 23, 17, 36, 34, 6, 12, 3, 48, 32, 4, 35, 10, 49, 4, 3, 50, 4, 51, 52, 5, 53, 54, 55, 6, 1, 13, 56, 2, 3, 57, 4, 58, 2, 59, 60, 61, 2, 62, 36, 15, 63, 64, 65, 2, 66, 67, 15, 68, 7, 69, 10, 70, 71, 72, 15, 3, 73, 74, 75, 1, 76, 77, 12, 78, 79, 80, 81, 15, 82, 37, 83, 3, 84, 4, 3, 85, 86, 1, 9], [8, 87, 88, 11, 7, 89, 24, 90, 91, 25, 18, 92, 1, 9], [8, 93, 94, 5, 14, 95, 26, 96, 6, 11, 7, 97, 19, 98, 1, 20, 99, 100, 17, 7, 101, 27, 102, 103, 1, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
