{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll preprocess the data and generate a plug_and_play pickle file for it\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: basic preprocessing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# packages used for processing: \n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# import the Text preprocessing helper to obtain the lists of field_name:content_word pairs\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.utils import *\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for the preprocessing script\n",
    "train_percentage = 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the related files and properly structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_content_words, field_words, content_words = prepare_input_data(data_files_paths['table_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Field_content_words: ', ['name hui', 'name jun', 'image <none>', 'imagesize <none>', 'caption <none>', 'fullname hui', 'fullname jun', 'education <none>', 'nationality <none>', 'playingstyle <none>', 'birthdate <none>', 'birthplace <none>', 'deathdate <none>', 'deathplace <none>', 'height <none>', 'weight <none>', 'medaltemplates <none>', 'articletitle hui', 'articletitle jun'])\n",
      "('Field_words: ', ['type', 'name', 'name', 'name', 'name', 'title', 'title', 'title', 'title', 'title'])\n",
      "('Content_words: ', ['pope', 'michael', 'iii', 'of', 'alexandria', '56th', 'pope', 'of', 'alexandria', '&'])\n"
     ]
    }
   ],
   "source": [
    "# check if all the three lists are proper by printing them out\n",
    "print(\"Field_content_words: \", field_content_words[1])\n",
    "print(\"Field_words: \", field_words[:10])\n",
    "print(\"Content_words: \", content_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 19, 99, 53, 40, 25, 59, 57, 37, 76]\n"
     ]
    }
   ],
   "source": [
    "# extract only the lenghts of the field_content_words and delete the field_content_words in order \n",
    "# to free up resources\n",
    "pair_lengths = map(lambda x: len(x), field_content_words)\n",
    "print(pair_lengths)\n",
    "del field_content_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_sentences = prepare_input_labels(data_files_paths['nb_sentences'], data_files_paths['train_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> pope michael iii of alexandria -lrb- also known as khail iii -rrb- was the coptic pope of alexandria and patriarch of the see of st. mark -lrb- 880 -- 907 -rrb- . in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community . this building was at one time believed to have later become the site of the cairo geniza . <eos>\n",
      "\n",
      "<start> hui jun is a male former table tennis player from china . <eos>\n",
      "\n",
      "<start> okan Öztürk -lrb- born 30 november 1977 -rrb- is a turkish professional footballer . he currently plays as a striker for yeni malatyaspor . <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label_sentences are concatenated properly to obtain the decoder sentences.\n",
    "for sent in label_sentences[:3]: print(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_field, field_dict, rev_field_dict, vocab_size_field = prepare_tokenizer(field_words)\n",
    "train_data_content, content_dict, rev_content_dict, vocab_size_content = prepare_tokenizer(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 106, 106)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[62],\n",
       "        [ 2],\n",
       "        [ 2]]), array([[11],\n",
       "        [37],\n",
       "        [38]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_size_field, len(rev_field_dict), len(field_dict))\n",
    "train_data_field[:3], train_data_content[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the group function to bring the data together:\n",
    "field_seq = np.squeeze(train_data_field).tolist()\n",
    "content_seq = np.squeeze(train_data_content).tolist()\n",
    "field_sequences, content_sequences = (group_tokenized_sequences(field_seq, pair_lengths),\n",
    "                                         group_tokenized_sequences(content_seq, pair_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<unk>', 1: 'years', 2: 'name', 3: 'birthplace', 4: 'birthdate', 5: 'caption', 6: 'articletitle', 7: 'image', 8: 'clubs', 9: 'deathplace', 10: 'teams', 11: 'label', 12: 'associatedacts', 13: 'title', 14: 'feastday', 15: 'caps', 16: 'goals', 17: 'statlabel', 18: 'deathdate', 19: 'beatifieddate', 20: 'residence', 21: 'pcupdate', 22: 'titles', 23: 'buried', 24: 'coach', 25: 'yearsactive', 26: 'spouse', 27: 'successor', 28: 'almamater', 29: 'fullname', 30: 'position', 31: 'predecessor', 32: 'religion', 33: 'imagesize', 34: 'debutteam', 35: 'statyear', 36: 'genre', 37: 'enthroned', 38: 'ended', 39: 'height', 40: 'event', 41: 'dateofhighestranking', 42: 'dateofcurrentranking', 43: 'updated', 44: 'team', 45: 'statvalue', 46: 'origin', 47: 'veneratedin', 48: 'beatifiedby', 49: 'patronage', 50: 'nationality', 51: 'weight', 52: 'medaltemplates', 53: 'currentclub', 54: 'plays', 55: 'website', 56: 'highestranking', 57: 'currentranking', 58: 'occupation', 59: 'number', 60: 'debutdate', 61: 'college', 62: 'type', 63: 'ordination', 64: 'consecration', 65: 'birthname', 66: 'signature', 67: 'education', 68: 'playingstyle', 69: 'clubnumber', 70: 'nickname', 71: 'country', 72: 'turnedpro', 73: 'retired', 74: 'racquet', 75: 'worldopenresult', 76: 'finals', 77: 'statehouse', 78: 'district', 79: 'termstart', 80: 'termend', 81: 'constituency', 82: 'party', 83: 'children', 84: 'alt', 85: 'bats', 86: 'throws', 87: 'debutyear', 88: 'awards', 89: 'background', 90: 'heightft', 91: 'heightin', 92: 'weightlbs', 93: 'undraftedyear', 94: 'stats', 95: 'databasefootball', 96: 'pfr', 97: 'probowls', 98: 'beatifiedplace', 99: 'canonizeddate', 100: 'canonizedplace', 101: 'canonizedby', 102: 'attributes', 103: 'majorshrine', 104: 'suppresseddate', 105: 'issues'}\n"
     ]
    }
   ],
   "source": [
    "print field_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 7, 5, 37, 37, 37, 38, 38, 38, 31, 31, 27, 27, 63, 64, 4, 65, 3, 18, 18, 18, 23, 23, 23, 23, 23, 23, 50, 32, 32, 32, 20, 20, 20, 20, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 28, 66, 6, 6, 6, 6, 6], [2, 2, 7, 33, 5, 29, 29, 67, 50, 68, 4, 3, 18, 9, 39, 51, 52, 6, 6]]\n",
      "[[11, 37, 38, 4, 20, 83, 11, 4, 20, 84, 85, 4, 7, 86, 4, 87, 39, 1, 1, 40, 8, 88, 12, 13, 41, 89, 42, 90, 42, 1, 1, 1, 1, 91, 12, 13, 41, 92, 4, 43, 93, 7, 94, 95, 44, 96, 97, 43, 39, 21, 45, 12, 13, 5, 14, 98, 22, 7, 44, 99, 6, 1, 1, 11, 37, 38, 4, 20], [23, 24, 1, 1, 1, 23, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "# print some slices of the field_sequences and the content_sequences:\n",
    "print(field_sequences[:2])\n",
    "print(content_sequences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the defined pad_sequences function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of padded_sequences: ', (10, 99))\n"
     ]
    }
   ],
   "source": [
    "padded_field_sequences = pad_sequences(field_sequences)\n",
    "print(\"Length of padded_sequences: \", padded_field_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform structuring of the label_sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: convert the label_sentences into a single flat list (order preserved) in order to tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 14, 26]\n"
     ]
    }
   ],
   "source": [
    "# extract the length information from the label_sentences\n",
    "label_sentences_lengths = map(lambda x: len(x.split()), label_sentences)\n",
    "print(label_sentences_lengths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'pope', 'michael', 'iii', 'of', 'alexandria', '-lrb-', 'also', 'known', 'as']\n"
     ]
    }
   ],
   "source": [
    "''' Warning: This is a huge map - reduce operation. And may take a long time to execute '''\n",
    "label_words_list = reduce(lambda x,y: x + y, map(lambda x: x.split(), label_sentences))\n",
    "print(label_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use the earlier defined function prepare_tokenizer to transform the input words into numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the tokenizer function to obtain the token and processed data for label_sentences\n",
    "train_data_label, label_dict, rev_label_dict, vocab_size_label = prepare_tokenizer(label_words_list)\n",
    "train_data_label = train_data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(461, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the group tokenized sequences function to restructure the tokenized input\n",
    "label_seq = np.squeeze(train_data_label).tolist()\n",
    "label_sequences = group_tokenized_sequences(label_seq, label_sentences_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, perform the pickling of the Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the structured dictionary to pickle in the pickle file:\n",
    "pickling_data = {\n",
    "    # ''' Input structured data: '''\n",
    "    \n",
    "    # field_encodings and related data:\n",
    "    'field_encodings': field_sequences,\n",
    "    'field_dict': field_dict,\n",
    "    'field_rev_dict': rev_field_dict,\n",
    "    'field_vocab_size': vocab_size_field,\n",
    "    \n",
    "    # content encodings and related data:\n",
    "    'content_encodings': content_sequences,\n",
    "    'content_dict': content_dict,\n",
    "    'content_rev_dict': rev_content_dict,\n",
    "    'content_vocab_size': vocab_size_content,\n",
    "    \n",
    "    \n",
    "    \n",
    "    #''' Label summary sentences: '''\n",
    "    \n",
    "    # label encodings and related data:\n",
    "    'label_encodings': label_sequences,\n",
    "    'label_dict': label_dict,\n",
    "    'label_rev_dict': rev_label_dict,\n",
    "    'label_vocab_size': vocab_size_label\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the function from this repository -> https://github.com/akanimax/machine-learning-helpers to perform pickling and unpickling. The code has been taken exactly and packaged in the Text_Preprocessing_Helpers module of this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been pickled at: ../Data/plug_and_play.pickle\n"
     ]
    }
   ],
   "source": [
    "# pickle the above defined dictionary at the plug_and_play_data_file path\n",
    "pickleIt(pickling_data, plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the purpose of this notebook is now complete. We can directly use this pickled data and start building the tensorflow graph to go forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See you in the graph building module! Asta la vista!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
