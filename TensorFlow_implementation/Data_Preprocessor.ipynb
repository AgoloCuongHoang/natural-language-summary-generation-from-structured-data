{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll preprocess the data and generate a plug_and_play pickle file for it\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: basic preprocessing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# packages used for processing: \n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# import the Text preprocessing helper to obtain the lists of field_name:content_word pairs\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.utils import *\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "Visualizations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for the preprocessing script\n",
    "train_percentage = 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the related files and properly structure it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_content_words, field_words, content_words = prepare_input_data(data_files_paths['table_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Field_content_words: ', ['name aaron', 'name hohlbein', 'image <none>', 'fullname aaron', 'fullname hohlbein', 'birthdate 16', 'birthdate august', 'birthdate 1985', 'birthplace middleton', 'birthplace ,', 'birthplace wisconsin', 'birthplace ,', 'birthplace united', 'birthplace states', 'height 6', 'height 0', 'position defender', 'currentclub <none>', 'clubnumber <none>', 'youthyears 2003', 'youthyears --', 'youthyears 2006', 'youthclubs wisconsin', 'youthclubs badgers', 'years 2003', 'years --', 'years 2006', 'years 2006', 'years 2007', 'years --', 'years 2010', 'years 2010', 'years 2011', 'clubs wisconsin', 'clubs badgers', 'clubs princeton', 'clubs 56ers', 'clubs kansas', 'clubs city', 'clubs wizards', 'clubs \\xe2\\x86\\x92', 'clubs miami', 'clubs fc', 'clubs -lrb-', 'clubs loan', 'clubs -rrb-', 'clubs fort', 'clubs lauderdale', 'clubs strikers', 'caps 12', 'caps 43', 'caps 10', 'caps 14', 'goals 0', 'goals 2', 'goals 0', 'goals 0', 'nationalyears <none>', 'nationalteam <none>', 'nationalcaps <none>', 'nationalgoals <none>', 'medaltemplates <none>', 'pcupdate june', 'pcupdate 4', 'pcupdate ,', 'pcupdate 2011', 'ntupdate <none>', 'articletitle aaron', 'articletitle hohlbein'])\n",
      "('Field_words: ', ['name', 'name', 'image', 'imagesize', 'caption', 'birthname', 'birthdate', 'birthplace', 'deathdate', 'deathplace'])\n",
      "('Content_words: ', ['walter', 'extra', '<none>', '<none>', '<none>', '<none>', '1954', '<none>', '<none>', '<none>'])\n"
     ]
    }
   ],
   "source": [
    "# check if all the three lists are proper by printing them out\n",
    "print(\"Field_content_words: \", field_content_words[1])\n",
    "print(\"Field_words: \", field_words[:10])\n",
    "print(\"Content_words: \", content_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract only the lenghts of the field_content_words and delete the field_content_words in order \n",
    "# to free up resources\n",
    "pair_lengths = map(lambda x: len(x), field_content_words)\n",
    "# print(pair_lengths)\n",
    "del field_content_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sentences = prepare_input_labels(data_files_paths['nb_sentences'], data_files_paths['train_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> walter extra is a german award-winning aerobatic pilot , chief aircraft designer and founder of extra flugzeugbau -lrb- extra aircraft construction -rrb- , a manufacturer of aerobatic aircraft . extra was trained as a mechanical engineer . he began his flight training in gliders , transitioning to powered aircraft to perform aerobatics . he built and flew a pitts special aircraft and later built his own extra ea-230 . extra began designing aircraft after competing in the 1982 world aerobatic championships . his aircraft constructions revolutionized the aerobatics flying scene and still dominate world competitions . the german pilot klaus schrodt won his world championship title flying an aircraft made by the extra firm . walter extra has designed a series of performance aircraft which include unlimited aerobatic aircraft and turboprop transports . <eos>\n",
      "\n",
      "<start> aaron hohlbein -lrb- born august 16 , 1985 in middleton , wisconsin -rrb- is an american soccer player who is currently without a club . <eos>\n",
      "\n",
      "<start> majda vrhovnik -lrb- nom de guerre lojzka -rrb- -lrb- 14 april 1922 -- 4 may 1945 -rrb- was a slovene communist and medical student . she was a member of the district committee of the communist party of slovenia for klagenfurt and was named a people 's hero of yugoslavia after her death . <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label_sentences are concatenated properly to obtain the decoder sentences.\n",
    "for sent in label_sentences[:3]: print(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_field, field_dict, rev_field_dict, vocab_size_field = prepare_tokenizer(field_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7540, 7540, 7540)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([4]), list([4]), list([6])], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab_size_field, len(rev_field_dict), len(field_dict))\n",
    "train_data_field[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the group function to bring the data together:\n",
    "field_seq = np.squeeze(train_data_field).tolist()\n",
    "field_sequences = group_tokenized_sequences(field_seq, pair_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print field_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some slices of the field_sequences and the content_sequences:\n",
    "print(field_sequences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the defined pad_sequences function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_field_sequences = pad_sequences(field_sequences)\n",
    "print(\"Length of padded_sequences: \", padded_field_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform structuring of the label_sentences and the content_words in order to create a unified vocabulary of it (for copy mechanism):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: convert the label_sentences into a single flat list (order preserved) in order to tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the length information from the label_sentences\n",
    "label_sentences_lengths = map(lambda x: len(x.split()), label_sentences)\n",
    "print(label_sentences_lengths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Warning: This is a huge map - reduce operation. And may take a long time to execute '''\n",
    "label_words_list = reduce(lambda x,y: x + y, map(lambda x: x.split(), label_sentences))\n",
    "print(label_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: store the lengths of the label_words_list and the content words in order to generate a unified vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_words_label_words_split_point = len(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the content_words and the label_words_list\n",
    "unified_sequence = content_words + label_words_list\n",
    "print(\"total length: \", len(unified_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the tokenizer for this purpose:\n",
    "temp, content_label_dict, rev_content_label_dict, vocab_size_content_label = prepare_tokenizer(unified_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now again split the two lists separately and finally group them together to obtain the final stuff\n",
    "content_seq = temp[: content_words_label_words_split_point]\n",
    "label_seq = temp[content_words_label_words_split_point: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the group tokenized sequences function to restructure the tokenized input\n",
    "content_seq = np.squeeze(content_seq).tolist()\n",
    "label_seq = np.squeeze(label_seq).tolist()\n",
    "\n",
    "content_sequences, label_sequences = (group_tokenized_sequences(content_seq, pair_lengths),\n",
    "                                          group_tokenized_sequences(label_seq, label_sentences_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the metadata file for the tensorboard_projector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata file path => Models/Metadata/\n",
    "metadata_path = os.path.join(base_model_path, \"Metadata\")\n",
    "print(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dot_vocab(field_dict, os.path.join(metadata_path, \"fields.vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dot_vocab(content_label_dict, os.path.join(metadata_path, \"content_labels.vocab\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, perform the pickling of the Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the structured dictionary to pickle in the pickle file:\n",
    "pickling_data = {\n",
    "    # ''' Input structured data: '''\n",
    "    \n",
    "    # field_encodings and related data:\n",
    "    'field_encodings': field_sequences,\n",
    "    'field_dict': field_dict,\n",
    "    'field_rev_dict': rev_field_dict,\n",
    "    'field_vocab_size': vocab_size_field,\n",
    "    \n",
    "    # content encodings and related data:\n",
    "    'content_encodings': content_sequences,\n",
    "    \n",
    "    # ''' Label summary sentences: '''\n",
    "    \n",
    "    # label encodings and related data:\n",
    "    'label_encodings': label_sequences,\n",
    "    \n",
    "    # V union C related data:\n",
    "    'content_union_label_dict': content_label_dict,\n",
    "    'rev_content_union_label_dict': rev_content_label_dict,\n",
    "    'content_label_vocab_size': vocab_size_content_label\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the function from this repository -> https://github.com/akanimax/machine-learning-helpers to perform pickling and unpickling. The code has been taken exactly and packaged in the Text_Preprocessing_Helpers module of this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the above defined dictionary at the plug_and_play_data_file path\n",
    "pickleIt(pickling_data, plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the purpose of this notebook is now complete. We can directly use this pickled data and start building the tensorflow graph to go forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See you in the graph building module! Asta la vista!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
