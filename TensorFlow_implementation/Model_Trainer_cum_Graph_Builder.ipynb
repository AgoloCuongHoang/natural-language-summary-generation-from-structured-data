{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll write the script for training the Order-Planner Model defined in the base referenced paper\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "link to paper -> https://arxiv.org/abs/1709.00155\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual, I'll start with the utility cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# The tensorflow_graph_package for this implementation\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *\n",
    "\n",
    "# import tensorflow temporarily:\n",
    "import tensorflow as tf\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for this script\n",
    "train_percentage = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickle the processed data file and create the train_dev pratitions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unPickleIt(plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_encodings = data['field_encodings']\n",
    "field_dict = data['field_dict']\n",
    "\n",
    "content_encodings = data['content_encodings']\n",
    "\n",
    "label_encodings = data['label_encodings']\n",
    "content_label_dict = data['content_union_label_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a randomized cell that prints a complete sample to verify the sanity of the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Contents: \n",
      "[('type', 'pope'), ('name', 'michael'), ('name', 'iii'), ('name', 'of'), ('name', 'alexandria'), ('title', '56th'), ('title', 'pope'), ('title', 'of'), ('title', 'alexandria'), ('title', '&'), ('title', 'patriarch'), ('title', 'of'), ('title', 'the'), ('title', 'see'), ('title', 'of'), ('title', 'st.'), ('title', 'mark'), ('image', '<none>'), ('caption', '<none>'), ('enthroned', '25'), ('enthroned', 'april'), ('enthroned', '880'), ('ended', '16'), ('ended', 'march'), ('ended', '907'), ('predecessor', 'shenouda'), ('predecessor', 'i'), ('successor', 'gabriel'), ('successor', 'i'), ('ordination', '<none>'), ('consecration', '<none>'), ('birthdate', '<none>'), ('birthname', '<none>'), ('birthplace', 'egypt'), ('deathdate', '16'), ('deathdate', 'march'), ('deathdate', '907'), ('buried', 'monastery'), ('buried', 'of'), ('buried', 'saint'), ('buried', 'macarius'), ('buried', 'the'), ('buried', 'great'), ('nationality', 'egyptian'), ('religion', 'coptic'), ('religion', 'orthodox'), ('religion', 'christian'), ('residence', 'saint'), ('residence', 'mark'), ('residence', \"'s\"), ('residence', 'church'), ('feastday', '16'), ('feastday', 'march'), ('feastday', '-lrb-'), ('feastday', '20'), ('feastday', 'baramhat'), ('feastday', 'in'), ('feastday', 'the'), ('feastday', 'coptic'), ('feastday', 'calendar'), ('feastday', '-rrb-'), ('almamater', '<none>'), ('signature', '<none>'), ('articletitle', 'pope'), ('articletitle', 'michael'), ('articletitle', 'iii'), ('articletitle', 'of'), ('articletitle', 'alexandria')]\n",
      "\n",
      "\n",
      "Summary: \n",
      "['<start>', 'pope', 'michael', 'iii', 'of', 'alexandria', '-lrb-', 'also', 'known', 'as', 'khail', 'iii', '-rrb-', 'was', 'the', 'coptic', 'pope', 'of', 'alexandria', 'and', 'patriarch', 'of', 'the', 'see', 'of', 'st.', 'mark', '-lrb-', '880', '--', '907', '-rrb-', '.', 'in', '882', ',', 'the', 'governor', 'of', 'egypt', ',', 'ahmad', 'ibn', 'tulun', ',', 'forced', 'khail', 'to', 'pay', 'heavy', 'contributions', ',', 'forcing', 'him', 'to', 'sell', 'a', 'church', 'and', 'some', 'attached', 'properties', 'to', 'the', 'local', 'jewish', 'community', '.', 'this', 'building', 'was', 'at', 'one', 'time', 'believed', 'to', 'have', 'later', 'become', 'the', 'site', 'of', 'the', 'cairo', 'geniza', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(field_encodings)\n",
    "\n",
    "random_index = np.random.randint(total_samples)\n",
    "\n",
    "# extract the three parts of this random sample\n",
    "random_field_sample = field_encodings[random_index]\n",
    "content_sample = content_encodings[random_index]\n",
    "label_sample = label_encodings[random_index]\n",
    "\n",
    "# print the extracted sample in meaningful format\n",
    "print(\"Table Contents: \")\n",
    "print([(field_dict[field], content_label_dict[content]) \n",
    "       for (field, content) in zip(random_field_sample, content_sample)])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Summary: \")\n",
    "print([content_label_dict[label] for label in label_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the above cell multiple times to satisfy yourself that the data is still sane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random shuffling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = synch_random_shuffle_non_np(zip(field_encodings, content_encodings), label_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform train_dev_splitting of the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, dev_X, dev_Y = split_train_dev(X, Y, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Examples in Training set: ', 9)\n",
      "('Number of Examples in the dev  set: ', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Examples in Training set: \", len(train_X))\n",
    "print(\"Number of Examples in the dev  set: \", len(dev_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph temporarily:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 0: Set the Hyper constants for the graph building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some hyper constants to be used in the graph building:\n",
    "\n",
    "# random_seed value for consistent debuggable behaviour\n",
    "seed_value = 3\n",
    "\n",
    "# vocabulary sizes\n",
    "field_vocab_size = data['field_vocab_size']\n",
    "content_label_vocab_size = data['content_label_vocab_size']\n",
    "\n",
    "# Embeddings size:\n",
    "field_embedding_size = 100\n",
    "content_label_embedding_size = 400 # This is a much bigger vocabulary compared to the field_name's vocabulary\n",
    "\n",
    "# LSTM hidden state sizes\n",
    "lstm_cell_state_size = hidden_state_size = 500 # they are same (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph reset point:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Create placeholders for the computations in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for the input data:\n",
    "with tf.variable_scope(\"Input_Data\"):\n",
    "    tf_field_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_field_encodings\")\n",
    "    tf_content_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_content_encodings\")\n",
    "    tf_label_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_label_encodings\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the input sequences (they are padded to tensor)\n",
    "    tf_input_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"input_sequence_lengths\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the decoder sequences (they are padded to tensor)\n",
    "    tf_label_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"decoder_sequence_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input_Data/input_field_encodings:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# check tf_field_encodings\n",
    "print(tf_field_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Obtain Embeddings for the input and the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scope for the shared Content_Label matrix\n",
    "with tf.variable_scope(\"Unified_Vocabulary_Matrix\"):\n",
    "    content_label_embedding_matrix = tf.get_variable(\"content_label_embedding_matrix\", \n",
    "                                shape=(content_label_vocab_size, content_label_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embeddings for the given input data:\n",
    "with tf.variable_scope(\"Input_Embedder\"):\n",
    "    # Embed the field encodings:\n",
    "    field_embedding_matrix = tf.get_variable(\"field_embedding_matrix\", \n",
    "                                shape=(field_vocab_size, field_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_field_embedded = tf.nn.embedding_lookup(field_embedding_matrix, tf_field_encodings, name=\"field_embedder\")\n",
    "    \n",
    "    # Embed the content encodings: \n",
    "    \n",
    "    \n",
    "    tf_content_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_content_encodings, name=\"content_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Input_Tensors: ', <tf.Tensor 'Input_Embedder/field_embedder:0' shape=(?, ?, 100) dtype=float32>, <tf.Tensor 'Input_Embedder/content_embedder:0' shape=(?, ?, 400) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Input_Tensors: \", tf_field_embedded, tf_content_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embeddings for the label (summary sentences):\n",
    "with tf.variable_scope(\"Label_Embedder\"):\n",
    "    # embed the label encodings\n",
    "    tf_label_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_label_encodings, name=\"label_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Label_Tensors: ', <tf.Tensor 'Label_Embedder/label_embedder:0' shape=(?, ?, 400) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Label_Tensors: \", tf_label_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the Input embeddings channel_wise and obtain the combined input tensor\n",
    "with tf.variable_scope(\"Input_Concatenator\"):\n",
    "    tf_field_content_embedded = tf.concat([tf_field_embedded, tf_content_embedded], axis=-1, name=\"concatenator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_Input_to_the_Encoder: ', <tf.Tensor 'Input_Concatenator/concatenator:0' shape=(?, ?, 500) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_Input_to_the_Encoder: \", tf_field_content_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Create the encoder RNN to obtain the encoded input sequences. <b>(The Encoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    encoded_input, encoder_final_state = tf.nn.dynamic_rnn (\n",
    "                            cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), # let all parameters to be default\n",
    "                            inputs = tf_field_content_embedded,\n",
    "                            sequence_length = tf_input_seqs_lengths,\n",
    "                            dtype = tf.float32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoded_vectors_bank for attention mechanism: ', <tf.Tensor 'Encoder/rnn/transpose:0' shape=(?, ?, 500) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded_vectors_bank for attention mechanism: \", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# define the size parameter for the encoded_inputs\n",
    "encoded_inputs_embeddings_size = encoded_input.shape[-1]\n",
    "print encoded_inputs_embeddings_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_state obtained from the last step of encoder: ', LSTMStateTuple(c=<tf.Tensor 'Encoder/rnn/while/Exit_2:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Encoder/rnn/while/Exit_3:0' shape=(?, 500) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_state obtained from the last step of encoder: \", encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4: define the Attention Mechanism for the Model <b>(The Dispatcher Module)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.1: define the content based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention\"):\n",
    "    '''\n",
    "        These weights and bias matrices must be compatible with the dimensions of the h_values and the f_values\n",
    "        passed to the function below. If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it. \n",
    "    '''\n",
    "    # field weights for the content_based attention\n",
    "    W_f = tf.get_variable(\"field_attention_weights\", shape=(field_embedding_size, content_label_embedding_size),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_f = tf.get_variable(\"field_attention_biases\", shape=(field_embedding_size,),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    \n",
    "    # hidden states weights for the content_based attention\n",
    "    W_c = tf.get_variable(\"content_attention_weights\", \n",
    "                          shape=(encoded_inputs_embeddings_size, content_label_embedding_size),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_c = tf.get_variable(\"content_attention_biases\", shape=(encoded_inputs_embeddings_size,),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention\"):\n",
    "    def get_content_based_attention_vectors(query_vectors):\n",
    "        '''\n",
    "            function that returns the alpha_content vector using the yt-1 (query vectors)\n",
    "        '''\n",
    "        # use the W_f and b_f to transform the query_vectors to the shape of f_values\n",
    "        f_trans_query_vectors = tf.matmul(W_f, tf.transpose(query_vectors)) + b_f\n",
    "        # use the W_c and b_c to transform the query_vectors to the shape of h_values\n",
    "        h_trans_query_vectors = tf.matmul(W_c, tf.transpose(query_vectors) + b_c)\n",
    "        \n",
    "        # use the tf.while_loop to obtain the required vectors\n",
    "        field_attention_values = tf.TensorArray(tf.float32, size=f_trans_query_vectors.shape[-1])\n",
    "        # use the tf.while_loop to obtain the required vectors\n",
    "        hidden_attention_values = tf.TensorArray(tf.float32, size=h_trans_query_vectors.shape[-1])\n",
    "        tf.while_loop (\n",
    "            lambda i, x, y: tf.less(i, field_attention_values.size()),\n",
    "            lambda i, x, y: (tf.add(i, 1), x.write(i, tf.matmul(tf_field_embedded[i, :, :],\n",
    "                                                        tf.transpose(tf.stack([f_trans_query_vectors[:, i]])))),\n",
    "                               y.write(i, tf.matmul(encoded_input[i, :, :], \n",
    "                                                        tf.transpose(tf.stack([h_trans_query_vectors[:, i]]))))),\n",
    "            [tf.constant(0), field_attention_values, hidden_attention_values],\n",
    "            name=\"attention_loop\"\n",
    "        )\n",
    "        \n",
    "        field_attention_values = tf.squeeze(field_attention_values.stack())\n",
    "        hidden_attention_values = tf.squeeze(hidden_attention_values.stack())\n",
    "        \n",
    "        # return the element wise multiplied values followed by softmax\n",
    "        return tf.nn.softmax(field_attention_values * hidden_attention_values, name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.2: define the link based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Link_Based_Attention\"):\n",
    "    '''\n",
    "        The dimensions of the Link_Matrix must be properly compatible with the field_vocab_size.\n",
    "        If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it.\n",
    "    '''\n",
    "    Link_Matrix = tf.get_variable(\"Link_Attention_Matrix\", shape=(field_vocab_size, field_vocab_size),\n",
    "            dtype=tf.float32, initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.5, seed=seed_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Link_Based_Attention/Link_Attention_Matrix:0' shape=(106, 106) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(Link_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for obtaining the link based attention values.\n",
    "with tf.variable_scope(\"Link_Based_Attention\"):\n",
    "    def get_link_based_attention_vectors(prev_attention_vectors):\n",
    "        '''\n",
    "            This function generates the link based attention vectors using the Link matrix and the \n",
    "        '''\n",
    "        # carve out only the relevant values from the Link matrix\n",
    "        \n",
    "        \n",
    "        matrix_all_values_from = tf.nn.embedding_lookup(Link_Matrix, tf_field_encodings)\n",
    "        matrix_relevant_values = tf.transpose(\n",
    "                        tf.gather_nd(tf.transpose(matrix_all_values_from, perm=[0, 2, 1]), tf_field_encodings), \n",
    "                        perm=[0, 2, 1])\n",
    "        \n",
    "        return tf.nn.softmax(tf.reduce_sum(prev_attention_vectors * matrix_relevant_values, axis=1),name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.3: define the hybrid attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hybrid of the content based and the link based attention\n",
    "with tf.variable_scope(\"Hybrid_attention\"):\n",
    "    # for now, this is just the content_based attention:\n",
    "    Zt_weights = tf.get_variable(\"zt_gate_parameter_vector\", dtype=tf.float32,\n",
    "                                 initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                 shape=(hidden_state_size + field_embedding_size + content_label_embedding_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Hybrid_attention\"):\n",
    "    # define the hybrid_attention_calculator function:\n",
    "    def get_hybrid_attention(h_values, y_values, content_attention, link_attention):\n",
    "        '''\n",
    "            function to calcuate the hybrid attention using the content_attention and the link_attention\n",
    "        '''\n",
    "        # calculate the e_f values\n",
    "        e_t = tf.reduce_sum(link_attention * tf_field_embedded, axis=1)\n",
    "        \n",
    "        # create the concatenated vectors from h_values e_t and y_values\n",
    "        input_to_zt_gate = tf.concat([h_values, e_t, y_values], axis=-1) # channel wise concatenation\n",
    "        \n",
    "        # perfrom the computations of the z gate:\n",
    "        z_t = tf.nn.sigmoid(tf.matmul(input_to_zt_gate, Zt_weights))\n",
    "        \n",
    "        # calculate z_t~ value using the empirical values = 0.2z_t + 0.5\n",
    "        z_t_tilde = (0.2 * z_t) + 0.5\n",
    "        \n",
    "        # compute the final hybrid_attention_values using the z_t_tilde values over content and link based values\n",
    "        hybrid_attention = (z_t_tilde * content_attention) + ((1 - z_t_tilde) * link_attention)\n",
    "        \n",
    "        # return the calculated hybrid attention:\n",
    "        return hybrid_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5: create the decoder RNN to obtain the generated summary for the structured data <b>(The Decoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.variable_scope(\"Decoder\"):\n",
    "        # define the weights for the output projection calculation\n",
    "        W_output = tf.get_variable(\n",
    "                            \"output_projector_matrix\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(hidden_state_size, content_label_vocab_size))\n",
    "        b_output = tf.get_variable(\n",
    "                            \"output_projector_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_vocab_size,))\n",
    "        \n",
    "        # define the weights and biases for the x_t calculation\n",
    "        W_d = tf.get_variable(\n",
    "                        \"x_t_gate_matrix\", dtype=tf.float32,\n",
    "                        initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                        shape=((hidden_state_size + content_label_embedding_size), content_label_embedding_size))\n",
    "        b_d = tf.get_variable(\n",
    "                            \"x_t_gate_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_embedding_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(start_tokens, mode = \"inference\", decoder_lengths = None, w_reuse = True):\n",
    "    '''\n",
    "        Function that defines the decoder op and returns the decoded sequence (the summary)\n",
    "        \n",
    "        @params:\n",
    "        start_tokens = a tensor containing the start tokens (one for each sequence in the batch)\n",
    "        mode = a value from \"training\" or \"inference\" to determine for how long the decoder rnn is to be unrolled.\n",
    "               behaviour is as follows:\n",
    "               \"training\" => The rnn will be unrolled until the max(decode_lengths). decode_lengths cannot be None.\n",
    "               \"inference\" => decode_lengths is be ignored and unrolling will be done till <eos> is received\n",
    "               \n",
    "    '''\n",
    "    with tf.variable_scope(\"Decoder\", reuse = w_reuse):\n",
    "        # define the function to obtain the predictions out of the given hidden_state_values\n",
    "        def get_predictions(h_t_values):\n",
    "            '''\n",
    "                This function transforms the h_t_values into a one_hot_type probability vector\n",
    "            '''\n",
    "            # apply the output_projection gate to obtain the predictions from the h_t_values\n",
    "            predictions = tf.matmul(h_t_values, W_output) + b_output\n",
    "            \n",
    "            # return the predictions:\n",
    "            return predictions\n",
    "        \n",
    "        \n",
    "        # define a function to obtain the values for the next input to the LSTM_cell (y_t values)\n",
    "        def get_y_t_values(pred_vals):\n",
    "            '''\n",
    "                pred_vals = the tensor of shape [batch_size x content_label_vocab_size]\n",
    "            '''\n",
    "            \n",
    "            # calculate the next words to be predicted \n",
    "            act_preds = tf.argmax(pred_vals, axis=-1)\n",
    "            \n",
    "            # perform embedding lookup for these act_preds\n",
    "            y_t_values = tf.nn.embedding_lookup(content_label_embedding_matrix, act_preds)\n",
    "            \n",
    "            # return the calculated y_t_values\n",
    "            return y_t_values\n",
    "            \n",
    "        \n",
    "        # write the loop function for the raw_rnn:\n",
    "        def decoder_loop_function(time, cell_output, cell_state, loop_state):\n",
    "            '''\n",
    "                The decoder loop function for the raw_rnn\n",
    "                (In future will implement the attention mechanism using the loop_state parameter.)\n",
    "                @params\n",
    "                compatible with -> https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn\n",
    "            '''\n",
    "            if(cell_state is None):\n",
    "                # initial call of the loop function\n",
    "                finished = (time >= tf_label_seqs_lengths)\n",
    "                next_input = start_tokens\n",
    "                next_cell_state = encoder_final_state\n",
    "                emit_output = None\n",
    "                next_loop_state = tf.zeros_like(tf_field_encodings, dtype=tf.float32)\n",
    "\n",
    "            else:\n",
    "                # we define the loop_state as the prev_hybrid attention_vector!\n",
    "                prev_attention_vectors = loop_state # extract the prev_attention_vector from the loop state\n",
    "                \n",
    "                # obtain the predictions for the cell_output\n",
    "                preds = get_predictions(cell_output)\n",
    "                \n",
    "                # obtain the y_t_values from the cell_output values:\n",
    "                y_t_values = get_y_t_values(preds)\n",
    "                \n",
    "                ''' Calculate the attention: '''\n",
    "                # calculate the content_based attention values using the defined module\n",
    "                cont_attn = get_content_based_attention_vectors(y_t_values)\n",
    "                \n",
    "                # calculate the link based attention values\n",
    "                link_attn = get_link_based_attention_vectors(prev_attention_vectors)\n",
    "                \n",
    "                # calculate the hybrid_attention\n",
    "                hybrid_attn = get_hybrid_attention(cell_output, y_t_values, cont_attn, link_attn)\n",
    "                \n",
    "                ''' Calculate the x_t vector for next_input value'''\n",
    "                # use the hybrid_attn to attend over the encoded_input (to calculate the a_t values)\n",
    "                a_t_values = tf.reduce_sum(hybrid_attn * encoded_input, axis=1) \n",
    "                \n",
    "                # apply the x_t gate\n",
    "                x_t = tf.tanh(tf.matmul(tf.concat([a_t_values, y_t_values], axis=-1), W_d) + b_d)\n",
    "                \n",
    "                \n",
    "                ''' Calculate the finished vector for perfoming computations '''\n",
    "                # for now it is just the decoder length completed or not value.\n",
    "                finished = (time >= decoder_lengths)\n",
    "                \n",
    "                ''' Copy mechanism is left (//TODO: change the following and implement copy mechanism)'''\n",
    "                emit_output = preds\n",
    "                \n",
    "                # The next_input is the x_t vector so calculated:\n",
    "                next_input = x_t\n",
    "                # The next loop_state is the current hybrid_attention vectors\n",
    "                next_loop_state = hybrid_attn\n",
    "                # The next_cell_state is going to be equal to the cell_state. (we_don't tweak it)\n",
    "                next_cell_state = cell_state\n",
    "            \n",
    "            # In both the cases, the return value is same.\n",
    "            # return all these created parameters\n",
    "            return (finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "        \n",
    "        # use the tf.nn.raw_rnn to define the decoder computations\n",
    "        outputs, _, _ = tf.nn.raw_rnn(tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), decoder_loop_function)\n",
    "        \n",
    "    # return the outputs obtained from the raw_rnn:\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step 6: define the training computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 1 in both shapes must be equal, but are 500 and 421 for 'Training_computations/Decoder/rnn/while/Select' (op: 'Select') with input shapes: [?], [?,500], [?,421].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-556-5aeecd10819f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training_computations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     outputs = decode(tf_label_embedded[:, 0, :], mode=\"training\", \n\u001b[0;32m----> 3\u001b[0;31m                      decoder_lengths=tf_label_seqs_lengths, w_reuse=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-555-d84efe47d8e0>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(start_tokens, mode, decoder_lengths, w_reuse)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# use the tf.nn.raw_rnn to define the decoder computations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_loop_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m# return the outputs obtained from the raw_rnn:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mraw_rnn\u001b[0;34m(cell, loop_fn, parallel_iterations, swap_memory, scope)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             emit_ta, state, loop_state],\n\u001b[1;32m   1081\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0memit_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loop_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2814\u001b[0m     \u001b[0mloop_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=redefined-outer-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2638\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2639\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2640\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2641\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2588\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2590\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mbody\u001b[0;34m(time, elements_finished, current_input, emit_ta, state, loop_state)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m       \u001b[0memit_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_copy_some_through\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_emit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m       \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_copy_some_through\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_copy_some_through\u001b[0;34m(current, candidate)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0memit_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_copy_some_through\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_emit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/util/nest.pyc\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 413\u001b[0;31m       structure[0], [func(*x) for x in entries])\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mcopy_fn\u001b[0;34m(cur_i, cand_i)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcopy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2441\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2442\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_select\u001b[0;34m(condition, t, e, name)\u001b[0m\n\u001b[1;32m   3986\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 3988\u001b[0;31m         \"Select\", condition=condition, t=t, e=e, name=name)\n\u001b[0m\u001b[1;32m   3989\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3990\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 500 and 421 for 'Training_computations/Decoder/rnn/while/Select' (op: 'Select') with input shapes: [?], [?,500], [?,421]."
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Training_computations\"):\n",
    "    outputs = decode(tf_label_embedded[:, 0, :], mode=\"training\", \n",
    "                     decoder_lengths=tf_label_seqs_lengths, w_reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _ : define the errands for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"): \n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stub_session to generate the graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Vocabulary_Matrix/content_label_embedding_matrix:0\n",
      "Input_Embedder/field_embedding_matrix:0\n",
      "Encoder/rnn/lstm_cell/kernel:0\n",
      "Encoder/rnn/lstm_cell/bias:0\n",
      "Content_Based_Attention/field_attention_weights:0\n",
      "Content_Based_Attention/field_attention_biases:0\n",
      "Content_Based_Attention/content_attention_weights:0\n",
      "Content_Based_Attention/content_attention_biases:0\n",
      "Link_Based_Attention/Link_Attention_Matrix:0\n",
      "Hybrid_attention/zt_gate_parameter_vector:0\n",
      "Decoder/output_projector_matrix:0\n",
      "Decoder/output_projector_biases:0\n",
      "Decoder/x_t_gate_matrix:0\n",
      "Decoder/x_t_gate_biases:0\n",
      "Training_computations/Decoder/rnn/lstm_cell/kernel:0\n",
      "Training_computations/Decoder/rnn/lstm_cell/bias:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensorboard_writer = tf.summary.FileWriter(model_path, graph=sess.graph, filename_suffix=\".bot\")\n",
    "    \n",
    "    # initialize the session to generate the visualization file\n",
    "    sess.run(init)\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "    \n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
