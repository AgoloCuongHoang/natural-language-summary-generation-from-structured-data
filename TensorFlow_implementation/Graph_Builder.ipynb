{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll write the script for building the Order-Planner Model defined in the base referenced paper.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "The jupyter allows for a very easy graph building process while using the tf.InteractiveSession(). It is almost as if we are using the eager execution strategy. [Note it is not exactly same as eager execution. we have to explicitly write <i> tenosr.eval() </i> for execution.]\n",
    "link to paper -> https://arxiv.org/abs/1709.00155\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual, I'll start with the utility cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# The tensorflow_graph_package for this implementation\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *\n",
    "\n",
    "# import tensorflow temporarily:\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for this script\n",
    "train_percentage = 90\n",
    "learning_rate = 3e-4 # for learning rate -> https://twitter.com/karpathy/status/801621764144971776?lang=en\n",
    "# I know the tweet was a joke, but I have noticed that this learning rate works quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickle the processed data file and create the train_dev pratitions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unPickleIt(plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_encodings = data['field_encodings']\n",
    "field_dict = data['field_dict']\n",
    "\n",
    "content_encodings = data['content_encodings']\n",
    "\n",
    "label_encodings = data['label_encodings']\n",
    "content_label_dict = data['content_union_label_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a randomized cell that prints a complete sample to verify the sanity of the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Contents: \n",
      "[('image', '<none>'), ('birthdate', '20'), ('birthdate', 'november'), ('birthdate', '1972'), ('birthplace', 'emporia'), ('birthplace', ','), ('birthplace', 'virginia'), ('position', 'defensive'), ('position', 'lineman'), ('number', '97'), ('college', 'north'), ('college', 'carolina'), ('heightft', '6'), ('heightin', '3'), ('weightlbs', '295'), ('undraftedyear', '1995'), ('stats', 'y'), ('databasefootball', 'parkerid01'), ('pfr', '<none>'), ('probowls', '<none>'), ('years', '1995\\xc2\\xa01996-2000\\xc2\\xa02001'), ('years', '2002-2003\\xc2\\xa02004'), ('teams', 'san'), ('teams', 'diego'), ('teams', 'chargers'), ('teams', 'seattle'), ('teams', 'seahawks'), ('teams', 'new'), ('teams', 'england'), ('teams', 'patriots'), ('teams', 'baltimore'), ('teams', 'ravens'), ('teams', 'san'), ('teams', 'francisco'), ('teams', '49ers'), ('articletitle', 'riddick'), ('articletitle', 'parker')]\n",
      "\n",
      "\n",
      "Summary: \n",
      "['<start>', 'riddick', 'parker', '-lrb-', 'born', 'november', '20', ',', '1972', 'in', 'emporia', ',', 'virginia', '-rrb-', 'is', 'a', 'former', 'professional', 'american', 'football', 'defensive', 'lineman', 'for', 'the', 'seattle', 'seahawks', ',', 'san', 'diego', 'chargers', ',', 'new', 'england', 'patriots', ',', 'baltimore', 'ravens', ',', 'and', 'san', 'francisco', '49ers', 'of', 'the', 'national', 'football', 'league', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(field_encodings)\n",
    "\n",
    "random_index = np.random.randint(total_samples)\n",
    "\n",
    "# extract the three parts of this random sample\n",
    "random_field_sample = field_encodings[random_index]\n",
    "content_sample = content_encodings[random_index]\n",
    "label_sample = label_encodings[random_index]\n",
    "\n",
    "# print the extracted sample in meaningful format\n",
    "print(\"Table Contents: \")\n",
    "print([(field_dict[field], content_label_dict[content]) \n",
    "       for (field, content) in zip(random_field_sample, content_sample)])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Summary: \")\n",
    "print([content_label_dict[label] for label in label_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the above cell multiple times to satisfy yourself that the data is still sane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random shuffling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = synch_random_shuffle_non_np(zip(field_encodings, content_encodings), label_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform train_dev_splitting of the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, dev_X, dev_Y = split_train_dev(X, Y, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_field, train_X_content = zip(*train_X)\n",
    "train_X_field = list(train_X_field); train_X_content = list(train_X_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Examples in Training set: ', 9)\n",
      "('Number of Examples in the dev  set: ', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Examples in Training set: \", len(train_X))\n",
    "print(\"Number of Examples in the dev  set: \", len(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free up the resources by deleting non required stuff\n",
    "del X, Y, field_encodings, content_encodings, train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the built graph will be later added to the code package Summary_Generator. This is being done here since the graph building process becomes quite easy with jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 0: Set the Hyper constants for the graph building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also put all the summary_ops along with the graph. While executing the graph we can decide whether we wish to generate the summary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set some hyper constants to be used in the graph building:\n",
    "\n",
    "# random_seed value for consistent debuggable behaviour\n",
    "seed_value = 3\n",
    "\n",
    "# vocabulary sizes\n",
    "field_vocab_size = data['field_vocab_size']\n",
    "content_label_vocab_size = data['content_label_vocab_size']\n",
    "\n",
    "# Embeddings size:\n",
    "field_embedding_size = 100\n",
    "content_label_embedding_size = 400 # This is a much bigger vocabulary compared to the field_name's vocabulary\n",
    "\n",
    "# LSTM hidden state sizes\n",
    "lstm_cell_state_size = hidden_state_size = 500 # they are same (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph reset point:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Create placeholders for the computations in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for the input data:\n",
    "with tf.variable_scope(\"Input_Data\"):\n",
    "    tf_field_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_field_encodings\")\n",
    "    tf_content_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_content_encodings\")\n",
    "    tf_label_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_label_encodings\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the input sequences (they are padded to tensor)\n",
    "    tf_input_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"input_sequence_lengths\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the decoder sequences (they are padded to tensor)\n",
    "    tf_label_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"decoder_sequence_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the one-hot encoded values for the label_encodings\n",
    "with tf.variable_scope(\"One_hot_encoder\"):\n",
    "    tf_one_hot_label_encodings = tf.one_hot(tf_label_encodings, depth=content_label_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input_Data/input_field_encodings:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# check tf_field_encodings\n",
    "print(tf_field_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Obtain Embeddings for the input and the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scope for the shared Content_Label matrix\n",
    "with tf.variable_scope(\"Unified_Vocabulary_Matrix\"):\n",
    "    content_label_embedding_matrix = tf.get_variable(\"content_label_embedding_matrix\", \n",
    "                                shape=(content_label_vocab_size, content_label_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embeddings for the given input data:\n",
    "with tf.variable_scope(\"Input_Embedder\"):\n",
    "    # Embed the field encodings:\n",
    "    field_embedding_matrix = tf.get_variable(\"field_embedding_matrix\", \n",
    "                                shape=(field_vocab_size, field_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_field_embedded = tf.nn.embedding_lookup(field_embedding_matrix, tf_field_encodings, name=\"field_embedder\")\n",
    "    \n",
    "    # Embed the content encodings: \n",
    "    \n",
    "    \n",
    "    tf_content_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_content_encodings, name=\"content_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Input_Tensors: ', <tf.Tensor 'Input_Embedder/field_embedder:0' shape=(?, ?, 100) dtype=float32>, <tf.Tensor 'Input_Embedder/content_embedder:0' shape=(?, ?, 400) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Input_Tensors: \", tf_field_embedded, tf_content_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embeddings for the label (summary sentences):\n",
    "with tf.variable_scope(\"Label_Embedder\"):\n",
    "    # embed the label encodings\n",
    "    tf_label_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_label_encodings, name=\"label_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Label_Tensors: ', <tf.Tensor 'Label_Embedder/label_embedder:0' shape=(?, ?, 400) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Label_Tensors: \", tf_label_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the Input embeddings channel_wise and obtain the combined input tensor\n",
    "with tf.variable_scope(\"Input_Concatenator\"):\n",
    "    tf_field_content_embedded = tf.concat([tf_field_embedded, tf_content_embedded], axis=-1, name=\"concatenator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_Input_to_the_Encoder: ', <tf.Tensor 'Input_Concatenator/concatenator:0' shape=(?, ?, 500) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_Input_to_the_Encoder: \", tf_field_content_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Create the encoder RNN to obtain the encoded input sequences. <b>(The Encoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    encoded_input, encoder_final_state = tf.nn.dynamic_rnn (\n",
    "                            cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), # let all parameters to be default\n",
    "                            inputs = tf_field_content_embedded,\n",
    "                            sequence_length = tf_input_seqs_lengths,\n",
    "                            dtype = tf.float32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoded_vectors_bank for attention mechanism: ', <tf.Tensor 'Encoder/rnn/transpose:0' shape=(?, ?, 500) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded_vectors_bank for attention mechanism: \", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# define the size parameter for the encoded_inputs\n",
    "encoded_inputs_embeddings_size = encoded_input.shape[-1]\n",
    "print encoded_inputs_embeddings_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_state obtained from the last step of encoder: ', LSTMStateTuple(c=<tf.Tensor 'Encoder/rnn/while/Exit_2:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Encoder/rnn/while/Exit_3:0' shape=(?, 500) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_state obtained from the last step of encoder: \", encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4: define the Attention Mechanism for the Model <b>(The Dispatcher Module)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.1: define the content based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention/trainable_weights\"):\n",
    "    '''\n",
    "        These weights and bias matrices must be compatible with the dimensions of the h_values and the f_values\n",
    "        passed to the function below. If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it. \n",
    "    '''\n",
    "    # field weights for the content_based attention\n",
    "    W_f = tf.get_variable(\"field_attention_weights\", shape=(field_embedding_size, content_label_embedding_size),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_f = tf.get_variable(\"field_attention_biases\", shape=(field_embedding_size, 1),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    \n",
    "    # hidden states weights for the content_based attention\n",
    "    W_c = tf.get_variable(\"content_attention_weights\", \n",
    "                          shape=(encoded_inputs_embeddings_size, content_label_embedding_size),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_c = tf.get_variable(\"content_attention_biases\", shape=(encoded_inputs_embeddings_size, 1),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    \n",
    "    # Define the summary_ops for all the weights:\n",
    "    W_f_summary = tf.summary.histogram(\"Content_based_attention/field_weights\", W_f)\n",
    "    b_f_summary = tf.summary.histogram(\"Content_based_attention/field_biases\", b_f)\n",
    "    W_c_summary = tf.summary.histogram(\"Content_based_attention/content_weights\", W_c)\n",
    "    b_c_summary = tf.summary.histogram(\"Content_based_attention/content_weights\", b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention\"):\n",
    "    def get_content_based_attention_vectors(query_vectors):\n",
    "        '''\n",
    "            function that returns the alpha_content vector using the yt-1 (query vectors)\n",
    "        '''\n",
    "        # use the W_f and b_f to transform the query_vectors to the shape of f_values\n",
    "        f_trans_query_vectors = tf.matmul(W_f, tf.transpose(query_vectors)) + b_f\n",
    "        # use the W_c and b_c to transform the query_vectors to the shape of h_values\n",
    "        h_trans_query_vectors = tf.matmul(W_c, tf.transpose(query_vectors)) + b_c\n",
    "        \n",
    "        # transpose and expand the dims of the f_trans_query_vectors\n",
    "        f_trans_query_matrices = tf.expand_dims(tf.transpose(f_trans_query_vectors), axis=-1)\n",
    "        # obtain the field attention_values by using the matmul operation\n",
    "        field_attention_values = tf.matmul(tf_field_embedded, f_trans_query_matrices)\n",
    "        \n",
    "        # perform the same process for the h_trans_query_vectors\n",
    "        h_trans_query_matrices = tf.expand_dims(tf.transpose(h_trans_query_vectors), axis=-1)\n",
    "        hidden_attention_values = tf.matmul(encoded_input, h_trans_query_matrices)\n",
    "        \n",
    "        # Don't use the squeeze operation! that will cause the loss of shape information\n",
    "        field_attention_values = field_attention_values[:, :, 0] # drop the last dimension (1 sized)\n",
    "        hidden_attention_values = hidden_attention_values[:, :, 0] # same for this one\n",
    "        \n",
    "        # return the element wise multiplied values followed by softmax\n",
    "        return tf.nn.softmax(field_attention_values * hidden_attention_values, name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.2: define the link based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Link_Based_Attention/trainable_weights\"):\n",
    "    '''\n",
    "        The dimensions of the Link_Matrix must be properly compatible with the field_vocab_size.\n",
    "        If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it.\n",
    "    '''\n",
    "    Link_Matrix = tf.get_variable(\"Link_Attention_Matrix\", shape=(field_vocab_size, field_vocab_size),\n",
    "            dtype=tf.float32, initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.5, seed=seed_value))\n",
    "    \n",
    "    Link_Matrix_summary = tf.summary.histogram(\"Link_based_attention\", Link_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Link_Based_Attention/trainable_weights/Link_Attention_Matrix:0' shape=(106, 106) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(Link_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the function for obtaining the link based attention values.\n",
    "with tf.variable_scope(\"Link_Based_Attention\"):\n",
    "    def get_link_based_attention_vectors(prev_attention_vectors):\n",
    "        '''\n",
    "            This function generates the link based attention vectors using the Link matrix and the \n",
    "        '''\n",
    "        # carve out only the relevant values from the Link matrix\n",
    "        matrix_all_values_from = tf.nn.embedding_lookup(Link_Matrix, tf_field_encodings)\n",
    "        \n",
    "        # // TODO: Calculate the matrix_relevant_values from matrix_all_values_from\n",
    "        matrix_relevant_values = tf.map_fn(lambda u: tf.gather(u[0],u[1],axis=1),\n",
    "                                [matrix_all_values_from, tf_field_encodings], dtype=matrix_all_values_from.dtype)\n",
    "        \n",
    "        \n",
    "        return tf.nn.softmax(tf.reduce_sum(tf.expand_dims(prev_attention_vectors, axis = -1) * \n",
    "                                           matrix_relevant_values, axis=1),name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.3: define the hybrid attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the hybrid of the content based and the link based attention\n",
    "with tf.variable_scope(\"Hybrid_attention/trainable_weights\"):\n",
    "    # for now, this is just the content_based attention:\n",
    "    Zt_weights = tf.get_variable(\"zt_gate_parameter_vector\", dtype=tf.float32,\n",
    "                                 initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                 shape=(hidden_state_size + field_embedding_size + content_label_embedding_size, 1))\n",
    "    \n",
    "    Zt_weights_summary = tf.summary.histogram(\"Hybrid_attention/zt_weights\", Zt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Hybrid_attention\"):\n",
    "    # define the hybrid_attention_calculator function:\n",
    "    def get_hybrid_attention(h_values, y_values, content_attention, link_attention):\n",
    "        '''\n",
    "            function to calcuate the hybrid attention using the content_attention and the link_attention\n",
    "        '''\n",
    "        # calculate the e_f values\n",
    "        e_t = tf.reduce_sum(tf.expand_dims(link_attention, axis=-1) * tf_field_embedded, axis=1)\n",
    "        \n",
    "        # create the concatenated vectors from h_values e_t and y_values\n",
    "        input_to_zt_gate = tf.concat([h_values, e_t, y_values], axis=-1) # channel wise concatenation\n",
    "        \n",
    "        # perfrom the computations of the z gate:\n",
    "        z_t = tf.nn.sigmoid(tf.matmul(input_to_zt_gate, Zt_weights))\n",
    "        \n",
    "        # calculate z_t~ value using the empirical values = 0.2z_t + 0.5\n",
    "        z_t_tilde = (0.2 * z_t) + 0.5\n",
    "        \n",
    "        # compute the final hybrid_attention_values using the z_t_tilde values over content and link based values\n",
    "        hybrid_attention = (z_t_tilde * content_attention) + ((1 - z_t_tilde) * link_attention)\n",
    "        \n",
    "        # return the calculated hybrid attention:\n",
    "        return hybrid_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5: create the decoder RNN to obtain the generated summary for the structured data <b>(The Decoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " with tf.variable_scope(\"Decoder/trainable_weights\"):\n",
    "        # define the weights for the output projection calculation\n",
    "        W_output = tf.get_variable(\n",
    "                            \"output_projector_matrix\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(hidden_state_size, content_label_vocab_size))\n",
    "        b_output = tf.get_variable(\n",
    "                            \"output_projector_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_vocab_size,))\n",
    "        \n",
    "        # define the weights and biases for the x_t calculation\n",
    "        W_d = tf.get_variable(\n",
    "                        \"x_t_gate_matrix\", dtype=tf.float32,\n",
    "                        initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                        shape=((hidden_state_size + content_label_embedding_size), content_label_embedding_size))\n",
    "        b_d = tf.get_variable(\n",
    "                            \"x_t_gate_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_embedding_size,))\n",
    "        \n",
    "        # define the summary ops for the defined weights and biases\n",
    "        W_output_summary = tf.summary.histogram(\"Decoder/W_output\", W_output)\n",
    "        b_output_summary = tf.summary.histogram(\"Decoder/b_output\", b_output)\n",
    "        W_d_summary = tf.summary.histogram(\"Decoder/W_d\", W_d)\n",
    "        b_d_summary = tf.summary.histogram(\"Decoder/b_d\", b_d)\n",
    "        \n",
    "        # create the LSTM cell to be used for decoding purposes\n",
    "        decoder_cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(start_tokens, mode = \"inference\", decoder_lengths = None, w_reuse = True):\n",
    "    '''\n",
    "        Function that defines the decoder op and returns the decoded sequence (the summary)\n",
    "        \n",
    "        @params:\n",
    "        start_tokens = a tensor containing the start tokens (one for each sequence in the batch)\n",
    "        mode = a value from \"training\" or \"inference\" to determine for how long the decoder rnn is to be unrolled.\n",
    "               behaviour is as follows:\n",
    "               \"training\" => The rnn will be unrolled until the max(decode_lengths). decode_lengths cannot be None.\n",
    "               \"inference\" => decode_lengths is be ignored and unrolling will be done till <eos> is received\n",
    "               \n",
    "    '''\n",
    "    with tf.variable_scope(\"Decoder\", reuse = w_reuse):\n",
    "        # define the function to obtain the predictions out of the given hidden_state_values\n",
    "        def get_predictions(h_t_values):\n",
    "            '''\n",
    "                This function transforms the h_t_values into a one_hot_type probability vector\n",
    "            '''\n",
    "            # apply the output_projection gate to obtain the predictions from the h_t_values\n",
    "            predictions = tf.matmul(h_t_values, W_output) + b_output\n",
    "            \n",
    "            # return the predictions:\n",
    "            return predictions\n",
    "        \n",
    "        \n",
    "        # define a function to obtain the values for the next input to the LSTM_cell (y_t values)\n",
    "        def get_y_t_values(pred_vals):\n",
    "            '''\n",
    "                pred_vals = the tensor of shape [batch_size x content_label_vocab_size]\n",
    "            '''\n",
    "            \n",
    "            # calculate the next words to be predicted \n",
    "            act_preds = tf.argmax(pred_vals, axis=-1)\n",
    "            \n",
    "            # perform embedding lookup for these act_preds\n",
    "            y_t_values = tf.nn.embedding_lookup(content_label_embedding_matrix, act_preds)\n",
    "            \n",
    "            # return the calculated y_t_values\n",
    "            return y_t_values\n",
    "            \n",
    "        \n",
    "        # write the loop function for the raw_rnn:\n",
    "        def decoder_loop_function(time, cell_output, cell_state, loop_state):\n",
    "            '''\n",
    "                The decoder loop function for the raw_rnn\n",
    "                (In future will implement the attention mechanism using the loop_state parameter.)\n",
    "                @params\n",
    "                compatible with -> https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn\n",
    "            '''\n",
    "            if(cell_state is None):\n",
    "                # initial call of the loop function\n",
    "                finished = (time >= tf_label_seqs_lengths)\n",
    "                next_input = start_tokens\n",
    "                next_cell_state = encoder_final_state\n",
    "                emit_output = tf.placeholder(tf.float32, shape=(content_label_vocab_size))\n",
    "                next_loop_state = tf.zeros_like(tf_field_encodings, dtype=tf.float32)\n",
    "                \n",
    "            else:\n",
    "                # we define the loop_state as the prev_hybrid attention_vector!\n",
    "                prev_attention_vectors = loop_state # extract the prev_attention_vector from the loop state\n",
    "                \n",
    "                # obtain the predictions for the cell_output\n",
    "                preds = get_predictions(cell_output)\n",
    "                \n",
    "                # obtain the y_t_values from the cell_output values:\n",
    "                y_t_values = get_y_t_values(preds)\n",
    "                \n",
    "                ''' Calculate the attention: '''\n",
    "                # calculate the content_based attention values using the defined module\n",
    "                cont_attn = get_content_based_attention_vectors(y_t_values)\n",
    "                \n",
    "                # calculate the link based attention values\n",
    "                link_attn = get_link_based_attention_vectors(prev_attention_vectors)\n",
    "                # print \"link_attention: \", link_attn\n",
    "                \n",
    "                # calculate the hybrid_attention\n",
    "                hybrid_attn = get_hybrid_attention(cell_output, y_t_values, cont_attn, link_attn)\n",
    "                \n",
    "                ''' Calculate the x_t vector for next_input value'''\n",
    "                # use the hybrid_attn to attend over the encoded_input (to calculate the a_t values)\n",
    "                a_t_values = tf.reduce_sum(tf.expand_dims(hybrid_attn, axis=-1) * encoded_input, axis=1) \n",
    "                \n",
    "                # apply the x_t gate\n",
    "                x_t = tf.tanh(tf.matmul(tf.concat([a_t_values, y_t_values], axis=-1), W_d) + b_d)\n",
    "                \n",
    "                \n",
    "                ''' Calculate the finished vector for perfoming computations '''\n",
    "                # for now it is just the decoder length completed or not value.\n",
    "                finished = (time >= decoder_lengths)\n",
    "                \n",
    "                ''' Copy mechanism is left (//TODO: change the following and implement copy mechanism)'''\n",
    "                emit_output = preds\n",
    "                \n",
    "                # The next_input is the x_t vector so calculated:\n",
    "                next_input = x_t\n",
    "                # The next loop_state is the current hybrid_attention vectors\n",
    "                next_loop_state = hybrid_attn\n",
    "                # The next_cell_state is going to be equal to the cell_state. (we_don't tweak it)\n",
    "                next_cell_state = cell_state\n",
    "            \n",
    "            # In both the cases, the return value is same.\n",
    "            # return all these created parameters\n",
    "            return (finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "        \n",
    "        # use the tf.nn.raw_rnn to define the decoder computations\n",
    "        outputs, _, _ = tf.nn.raw_rnn(decoder_cell, decoder_loop_function)\n",
    "        \n",
    "    # return the outputs obtained from the raw_rnn:\n",
    "    return tf.transpose(outputs.stack(), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step 6: define the training computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Training_computations\"):\n",
    "    outputs = decode(tf_label_embedded[:, 0, :], mode=\"training\", \n",
    "                     decoder_lengths=tf_label_seqs_lengths, w_reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Output_tensor: ', <tf.Tensor 'Training_computations/transpose:0' shape=(?, ?, 421) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# print the outputs:\n",
    "print(\"Output_tensor: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step 7: define the cost function and the optimizer to perform the optimization on this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss (objective) function for minimization\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf_one_hot_label_encodings))\n",
    "    \n",
    "    # record the loss summary:\n",
    "    loss_summary = tf.summary.scalar(\"Objective_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the optimizer for this task:\n",
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # define the train_step for this:\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _ : define the errands for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"): \n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stub_session to generate the graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Vocabulary_Matrix/content_label_embedding_matrix:0\n",
      "Input_Embedder/field_embedding_matrix:0\n",
      "Encoder/rnn/lstm_cell/kernel:0\n",
      "Encoder/rnn/lstm_cell/bias:0\n",
      "Content_Based_Attention/trainable_weights/field_attention_weights:0\n",
      "Content_Based_Attention/trainable_weights/field_attention_biases:0\n",
      "Content_Based_Attention/trainable_weights/content_attention_weights:0\n",
      "Content_Based_Attention/trainable_weights/content_attention_biases:0\n",
      "Link_Based_Attention/trainable_weights/Link_Attention_Matrix:0\n",
      "Hybrid_attention/trainable_weights/zt_gate_parameter_vector:0\n",
      "Decoder/trainable_weights/output_projector_matrix:0\n",
      "Decoder/trainable_weights/output_projector_biases:0\n",
      "Decoder/trainable_weights/x_t_gate_matrix:0\n",
      "Decoder/trainable_weights/x_t_gate_biases:0\n",
      "Decoder/rnn/lstm_cell/kernel:0\n",
      "Decoder/rnn/lstm_cell/bias:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensorboard_writer = tf.summary.FileWriter(model_path, graph=sess.graph, filename_suffix=\".bot\")\n",
    "    \n",
    "    # initialize the session to generate the visualization file\n",
    "    sess.run(init)\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "    \n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the session runner to check if the training loops execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell ensures that although there are no errors in the graph compilation, the runtime execution of the model also doesn't cause any problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the projector's configuration to add the embedding summary also:\n",
    "conf = projector.ProjectorConfig()\n",
    "embedding_field = conf.embeddings.add()\n",
    "embedding_content_label = conf.embeddings.add()\n",
    "\n",
    "# set the tensors to these embedding matrices\n",
    "embedding_field.tensor_name = field_embedding_matrix.name\n",
    "embedding_content_label.tensor_name = content_label_embedding_matrix.name\n",
    "\n",
    "# add the metadata paths to these embedding_summaries:\n",
    "embedding_field.metadata_path = os.path.join(\"..\", \"Metadata/fields.vocab\")\n",
    "embedding_content_label.metadata_path = os.path.join(\"..\", \"Metadata/content_labels.vocab\")\n",
    "\n",
    "# save the configuration file for this\n",
    "projector.visualize_embeddings(tensorboard_writer, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_epoch:  1\n",
      "Cost:  7.27336 \n",
      "\n",
      "\n",
      "current_epoch:  2\n",
      "Cost:  6.97948 \n",
      "\n",
      "\n",
      "current_epoch:  3\n",
      "Cost:  7.04352 \n",
      "\n",
      "\n",
      "current_epoch:  4\n",
      "Cost:  7.18313 \n",
      "\n",
      "\n",
      "current_epoch:  5\n",
      "Cost:  6.73581 \n",
      "\n",
      "\n",
      "current_epoch:  6\n",
      "Cost:  6.64475 \n",
      "\n",
      "\n",
      "current_epoch:  7\n",
      "Cost:  6.6705 \n",
      "\n",
      "\n",
      "current_epoch:  8\n",
      "Cost:  6.49349 \n",
      "\n",
      "\n",
      "current_epoch:  9\n",
      "Cost:  6.30597 \n",
      "\n",
      "\n",
      "current_epoch:  10\n",
      "Cost:  6.29757 \n",
      "\n",
      "\n",
      "current_epoch:  11\n",
      "Cost:  6.34497 \n",
      "\n",
      "\n",
      "current_epoch:  12\n",
      "Cost:  6.27614 \n",
      "\n",
      "\n",
      "current_epoch:  13\n",
      "Cost:  6.21918 \n",
      "\n",
      "\n",
      "current_epoch:  14\n",
      "Cost:  6.0565 \n",
      "\n",
      "\n",
      "current_epoch:  15\n",
      "Cost:  6.10464 \n",
      "\n",
      "\n",
      "current_epoch:  16\n",
      "Cost:  5.99213 \n",
      "\n",
      "\n",
      "current_epoch:  17\n",
      "Cost:  5.88979 \n",
      "\n",
      "\n",
      "current_epoch:  18\n",
      "Cost:  5.87693 \n",
      "\n",
      "\n",
      "current_epoch:  19\n",
      "Cost:  5.75283 \n",
      "\n",
      "\n",
      "current_epoch:  20\n",
      "Cost:  5.66356 \n",
      "\n",
      "\n",
      "current_epoch:  21\n",
      "Cost:  5.60344 \n",
      "\n",
      "\n",
      "current_epoch:  22\n",
      "Cost:  5.55368 \n",
      "\n",
      "\n",
      "current_epoch:  23\n",
      "Cost:  5.59065 \n",
      "\n",
      "\n",
      "current_epoch:  24\n",
      "Cost:  5.59801 \n",
      "\n",
      "\n",
      "current_epoch:  25\n",
      "Cost:  5.46834 \n",
      "\n",
      "\n",
      "current_epoch:  26\n",
      "Cost:  5.4536 \n",
      "\n",
      "\n",
      "current_epoch:  27\n",
      "Cost:  5.35707 \n",
      "\n",
      "\n",
      "current_epoch:  28\n",
      "Cost:  5.42687 \n",
      "\n",
      "\n",
      "current_epoch:  29\n",
      "Cost:  5.3388 \n",
      "\n",
      "\n",
      "current_epoch:  30\n",
      "Cost:  5.46349 \n",
      "\n",
      "\n",
      "current_epoch:  31\n",
      "Cost:  5.28002 \n",
      "\n",
      "\n",
      "current_epoch:  32\n",
      "Cost:  5.22819 \n",
      "\n",
      "\n",
      "current_epoch:  33\n",
      "Cost:  5.20142 \n",
      "\n",
      "\n",
      "current_epoch:  34\n",
      "Cost:  5.12881 \n",
      "\n",
      "\n",
      "current_epoch:  35\n",
      "Cost:  5.10172 \n",
      "\n",
      "\n",
      "current_epoch:  36\n",
      "Cost:  5.08573 \n",
      "\n",
      "\n",
      "current_epoch:  37\n",
      "Cost:  5.09548 \n",
      "\n",
      "\n",
      "current_epoch:  38\n",
      "Cost:  5.11359 \n",
      "\n",
      "\n",
      "current_epoch:  39\n",
      "Cost:  5.09625 \n",
      "\n",
      "\n",
      "current_epoch:  40\n",
      "Cost:  5.05523 \n",
      "\n",
      "\n",
      "current_epoch:  41\n",
      "Cost:  5.00976 \n",
      "\n",
      "\n",
      "current_epoch:  42\n",
      "Cost:  4.97366 \n",
      "\n",
      "\n",
      "current_epoch:  43\n",
      "Cost:  4.8903 \n",
      "\n",
      "\n",
      "current_epoch:  44\n",
      "Cost:  4.89832 \n",
      "\n",
      "\n",
      "current_epoch:  45\n",
      "Cost:  4.85829 \n",
      "\n",
      "\n",
      "current_epoch:  46\n",
      "Cost:  4.81368 \n",
      "\n",
      "\n",
      "current_epoch:  47\n",
      "Cost:  4.85057 \n",
      "\n",
      "\n",
      "current_epoch:  48\n",
      "Cost:  4.85571 \n",
      "\n",
      "\n",
      "current_epoch:  49\n",
      "Cost:  4.84478 \n",
      "\n",
      "\n",
      "current_epoch:  50\n",
      "Cost:  4.81976 \n",
      "\n",
      "\n",
      "current_epoch:  51\n",
      "Cost:  4.75791 \n",
      "\n",
      "\n",
      "current_epoch:  52\n",
      "Cost:  4.69033 \n",
      "\n",
      "\n",
      "current_epoch:  53\n",
      "Cost:  4.81211 \n",
      "\n",
      "\n",
      "current_epoch:  54\n",
      "Cost:  4.71743 \n",
      "\n",
      "\n",
      "current_epoch:  55\n",
      "Cost:  4.66858 \n",
      "\n",
      "\n",
      "current_epoch:  56\n",
      "Cost:  4.6554 \n",
      "\n",
      "\n",
      "current_epoch:  57\n",
      "Cost:  4.61168 \n",
      "\n",
      "\n",
      "current_epoch:  58\n",
      "Cost:  4.64289 \n",
      "\n",
      "\n",
      "current_epoch:  59\n",
      "Cost:  4.80116 \n",
      "\n",
      "\n",
      "current_epoch:  60\n",
      "Cost:  4.76036 \n",
      "\n",
      "\n",
      "current_epoch:  61\n",
      "Cost:  4.70557 \n",
      "\n",
      "\n",
      "current_epoch:  62\n",
      "Cost:  4.63745 \n",
      "\n",
      "\n",
      "current_epoch:  63\n",
      "Cost:  4.63279 \n",
      "\n",
      "\n",
      "current_epoch:  64\n",
      "Cost:  4.61019 \n",
      "\n",
      "\n",
      "current_epoch:  65\n",
      "Cost:  4.61805 \n",
      "\n",
      "\n",
      "current_epoch:  66\n",
      "Cost:  4.59636 \n",
      "\n",
      "\n",
      "current_epoch:  67\n",
      "Cost:  4.5864 \n",
      "\n",
      "\n",
      "current_epoch:  68\n",
      "Cost:  4.54833 \n",
      "\n",
      "\n",
      "current_epoch:  69\n",
      "Cost:  4.61963 \n",
      "\n",
      "\n",
      "current_epoch:  70\n",
      "Cost:  4.53445 \n",
      "\n",
      "\n",
      "current_epoch:  71\n",
      "Cost:  4.48489 \n",
      "\n",
      "\n",
      "current_epoch:  72\n",
      "Cost:  4.49962 \n",
      "\n",
      "\n",
      "current_epoch:  73\n",
      "Cost:  4.52575 \n",
      "\n",
      "\n",
      "current_epoch:  74\n",
      "Cost:  4.57339 \n",
      "\n",
      "\n",
      "current_epoch:  75\n",
      "Cost:  4.53498 \n",
      "\n",
      "\n",
      "current_epoch:  76\n",
      "Cost:  4.47895 \n",
      "\n",
      "\n",
      "current_epoch:  77\n",
      "Cost:  4.54662 \n",
      "\n",
      "\n",
      "current_epoch:  78\n",
      "Cost:  4.46181 \n",
      "\n",
      "\n",
      "current_epoch:  79\n",
      "Cost:  4.40223 \n",
      "\n",
      "\n",
      "current_epoch:  80\n",
      "Cost:  4.40978 \n",
      "\n",
      "\n",
      "current_epoch:  81\n",
      "Cost:  4.37639 \n",
      "\n",
      "\n",
      "current_epoch:  82\n",
      "Cost:  4.38006 \n",
      "\n",
      "\n",
      "current_epoch:  83\n",
      "Cost:  4.30396 \n",
      "\n",
      "\n",
      "current_epoch:  84\n",
      "Cost:  4.32778 \n",
      "\n",
      "\n",
      "current_epoch:  85\n",
      "Cost:  4.30964 \n",
      "\n",
      "\n",
      "current_epoch:  86\n",
      "Cost:  4.3049 \n",
      "\n",
      "\n",
      "current_epoch:  87\n",
      "Cost:  4.3462 \n",
      "\n",
      "\n",
      "current_epoch:  88\n",
      "Cost:  4.2941 \n",
      "\n",
      "\n",
      "current_epoch:  89\n",
      "Cost:  4.36495 \n",
      "\n",
      "\n",
      "current_epoch:  90\n",
      "Cost:  4.2951 \n",
      "\n",
      "\n",
      "current_epoch:  91\n",
      "Cost:  4.29672 \n",
      "\n",
      "\n",
      "current_epoch:  92\n",
      "Cost:  4.28135 \n",
      "\n",
      "\n",
      "current_epoch:  93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f2caa9d6f16d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtf_label_encodings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtf_input_seqs_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtf_label_seqs_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlab_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         })\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/animesh/Programming/platforms/anaconda3/envs/snakes/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' The following is just a runtime checker session loop. This loop is not the training loop for the model.\n",
    "Which is the reason why, the model is not saved upon executing'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a saver object:\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the saved weights:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    else:\n",
    "        # run the initializer to create the variables\n",
    "        sess.run(init)\n",
    "    \n",
    "    # obtain the padded training data:\n",
    "    inp_field = pad_sequences(train_X_field)\n",
    "    inp_conte = pad_sequences(train_X_content)\n",
    "    inp_label = pad_sequences(train_Y)\n",
    "    # print inp_field.shape, inp_conte.shape, inp_label.shape\n",
    "    \n",
    "    # obtain the sequence lengths for the field_encodings and the label_encodings\n",
    "    inp_lengths = get_lengths(train_X_field)\n",
    "    lab_lengths = get_lengths(train_Y)\n",
    "    # print inp_lengths, lab_lengths\n",
    "    \n",
    "    # run a loop for 1000 iterations:\n",
    "    for epoch in range(1000):\n",
    "        print \"current_epoch: \", (epoch + 1)\n",
    "        # execute the cost and the train_step\n",
    "        predicts, _, cost = sess.run([outputs, train_step, loss], feed_dict = {\n",
    "            tf_field_encodings: inp_field,\n",
    "            tf_content_encodings: inp_conte,\n",
    "            tf_label_encodings: inp_label,\n",
    "            tf_input_seqs_lengths: inp_lengths,\n",
    "            tf_label_seqs_lengths: lab_lengths\n",
    "        })\n",
    "        \n",
    "        if((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            # generate the summary for this batch:\n",
    "            sums = sess.run(all_summaries, feed_dict = {\n",
    "                tf_field_encodings: inp_field,\n",
    "                tf_content_encodings: inp_conte,\n",
    "                tf_label_encodings: inp_label,\n",
    "                tf_input_seqs_lengths: inp_lengths,\n",
    "                tf_label_seqs_lengths: lab_lengths\n",
    "            })\n",
    "            \n",
    "            # save this generated summary to the summary file\n",
    "            tensorboard_writer.add_summary(sums, global_step=(epoch + 1))\n",
    "            \n",
    "            # also save the model \n",
    "            saver.save(sess, os.path.join(model_path, model_name), global_step=(epoch + 1))\n",
    "            \n",
    "        print \"Cost: \", cost, \"\\n\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
