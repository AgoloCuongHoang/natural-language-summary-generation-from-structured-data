{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll write the script for training the Order-Planner Model defined in the base referenced paper\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "link to paper -> https://arxiv.org/abs/1709.00155\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual, I'll start with the utility cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# The tensorflow_graph_package for this implementation\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *\n",
    "\n",
    "# import tensorflow temporarily:\n",
    "import tensorflow as tf\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Literature\n",
      "README.md\n",
      "Scripts\n",
      "TensorFlow_implementation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for this script\n",
    "train_percentage = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickle the processed data file and create the train_dev pratitions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = unPickleIt(plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "field_encodings = data['field_encodings']\n",
    "field_dict = data['field_dict']\n",
    "\n",
    "content_encodings = data['content_encodings']\n",
    "content_dict = data['content_dict']\n",
    "\n",
    "label_encodings = data['label_encodings']\n",
    "label_dict = data['label_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a randomized cell that prints a complete sample to verify the sanity of the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Contents: \n",
      "[('image', '<none>'), ('birthdate', '20'), ('birthdate', 'november'), ('birthdate', '1972'), ('birthplace', 'emporia'), ('birthplace', ','), ('birthplace', 'virginia'), ('position', 'defensive'), ('position', 'lineman'), ('number', '97'), ('college', 'north'), ('college', 'carolina'), ('heightft', '6'), ('heightin', '3'), ('weightlbs', '295'), ('undraftedyear', '1995'), ('stats', 'y'), ('databasefootball', 'parkerid01'), ('pfr', '<none>'), ('probowls', '<none>'), ('years', '1995\\xc2\\xa01996-2000\\xc2\\xa02001'), ('years', '2002-2003\\xc2\\xa02004'), ('teams', 'san'), ('teams', 'diego'), ('teams', 'chargers'), ('teams', 'seattle'), ('teams', 'seahawks'), ('teams', 'new'), ('teams', 'england'), ('teams', 'patriots'), ('teams', 'baltimore'), ('teams', 'ravens'), ('teams', 'san'), ('teams', 'francisco'), ('teams', '49ers'), ('articletitle', 'riddick'), ('articletitle', 'parker')]\n",
      "\n",
      "\n",
      "Summary: \n",
      "['<start>', 'riddick', 'parker', '-lrb-', 'born', 'november', '20', ',', '1972', 'in', 'emporia', ',', 'virginia', '-rrb-', 'is', 'a', 'former', 'professional', 'american', 'football', 'defensive', 'lineman', 'for', 'the', 'seattle', 'seahawks', ',', 'san', 'diego', 'chargers', ',', 'new', 'england', 'patriots', ',', 'baltimore', 'ravens', ',', 'and', 'san', 'francisco', '49ers', 'of', 'the', 'national', 'football', 'league', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(field_encodings)\n",
    "\n",
    "random_index = np.random.randint(total_samples)\n",
    "\n",
    "# extract the three parts of this random sample\n",
    "random_field_sample = field_encodings[random_index]\n",
    "content_sample = content_encodings[random_index]\n",
    "label_sample = label_encodings[random_index]\n",
    "\n",
    "# print the extracted sample in meaningful format\n",
    "print(\"Table Contents: \")\n",
    "print([(field_dict[field], content_dict[content]) for (field, content) in zip(random_field_sample, content_sample)])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Summary: \")\n",
    "print([label_dict[label] for label in label_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the above cell multiple times to satisfy yourself that the data is still sane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random shuffling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = synch_random_shuffle_non_np(zip(field_encodings, content_encodings), label_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform train_dev_splitting of the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, dev_X, dev_Y = split_train_dev(X, Y, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Examples in Training set: ', 9)\n",
      "('Number of Examples in the dev  set: ', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Examples in Training set: \", len(train_X))\n",
    "print(\"Number of Examples in the dev  set: \", len(dev_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph temporarily:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 0: Set the Hyper constants for the graph building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyper constants to be used in the graph building:\n",
    "\n",
    "# random_seed value for consistent debuggable behaviour\n",
    "seed_value = 3\n",
    "\n",
    "# vocabulary sizes\n",
    "field_vocab_size = len(field_dict)\n",
    "content_vocab_size = len(content_dict)\n",
    "label_vocab_size = len(label_dict)\n",
    "\n",
    "# Embeddings size:\n",
    "field_embedding_size = content_embedding_size = 256\n",
    "label_embedding_size = 256 # this is same as the other two (for now)\n",
    "\n",
    "# LSTM hidden state sizes\n",
    "lstm_cell_state_size = hidden_state_size = 512 # they are same (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph reset point:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Create placeholders for the computations in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for the input data:\n",
    "with tf.variable_scope(\"Input_data\"):\n",
    "    tf_field_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_field_encodings\")\n",
    "    tf_content_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_content_encodings\")\n",
    "    tf_label_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_label_encodings\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the input sequences (they are padded to tensor)\n",
    "    tf_input_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"input_sequence_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input_data/input_field_encodings:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# check tf_field_encodings\n",
    "print(tf_field_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Obtain Embeddings for the input and the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for the given input data:\n",
    "with tf.variable_scope(\"Input_Embedder\"):\n",
    "    # Embed the field encodings:\n",
    "    field_embedding_matrix = tf.get_variable(\"field_embedding_matrix\", \n",
    "                                shape=(field_vocab_size, field_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_field_embedded = tf.nn.embedding_lookup(field_embedding_matrix, tf_field_encodings, name=\"field_embedder\")\n",
    "    \n",
    "    # Embed the content encodings: \n",
    "    content_embedding_matrix = tf.get_variable(\"content_embedding_matrix\", \n",
    "                                shape=(content_vocab_size, content_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_content_embedded = tf.nn.embedding_lookup(content_embedding_matrix, \n",
    "                                                 tf_content_encodings, name=\"content_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Input_Tensors: ', <tf.Tensor 'Input_Embedder/field_embedder:0' shape=(?, ?, 256) dtype=float32>, <tf.Tensor 'Input_Embedder/content_embedder:0' shape=(?, ?, 256) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Input_Tensors: \", tf_field_embedded, tf_content_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embeddings for the label (summary sentences):\n",
    "with tf.variable_scope(\"Label_Embedder\"):\n",
    "    # Embed the label encodings: \n",
    "    label_embedding_matrix = tf.get_variable(\"label_embedding_matrix\", \n",
    "                                shape=(label_vocab_size, label_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_label_embedded = tf.nn.embedding_lookup(label_embedding_matrix, \n",
    "                                                 tf_label_encodings, name=\"label_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedded_Label_Tensors: ', <tf.Tensor 'Label_Embedder/label_embedder:0' shape=(?, ?, 256) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Label_Tensors: \", tf_label_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the Input embeddings channel_wise and obtain the combined input tensor\n",
    "with tf.variable_scope(\"Input_Concatenator\"):\n",
    "    tf_field_content_embedded = tf.concat([tf_field_embedded, tf_content_embedded], axis=-1, name=\"concatenator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_Input_to_the_Encoder: ', <tf.Tensor 'Input_Concatenator/concatenator:0' shape=(?, ?, 512) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_Input_to_the_Encoder: \", tf_field_content_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Create the encoder RNN to obtain the encoded input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    encoded_input, final_state = tf.nn.dynamic_rnn (\n",
    "                            cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), # let all parameters to be default\n",
    "                            inputs = tf_field_content_embedded,\n",
    "                            sequence_length = tf_input_seqs_lengths,\n",
    "                            dtype = tf.float32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoded_vectors_bank for attention mechanism: ', <tf.Tensor 'Encoder/rnn/transpose:0' shape=(?, ?, 512) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded_vectors_bank for attention mechanism: \", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final_state obtained from the last step of encoder: ', LSTMStateTuple(c=<tf.Tensor 'Encoder/rnn/while/Exit_2:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'Encoder/rnn/while/Exit_3:0' shape=(?, 512) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_state obtained from the last step of encoder: \", final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stub_session to generate the graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensorboard_writer = tf.summary.FileWriter(model_path, graph=sess.graph, filename_suffix=\".bot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
